// Start a Spark Session
import org.apache.spark.sql.SparkSession

// Optional: Use the following code below to set the Error reporting
import org.apache.log4j._
Logger.getLogger("org").setLevel(Level.ERROR)

// Spark Session
val spark = SparkSession.builder().getOrCreate()

// Import clustering Algorithm
import org.apache.spark.ml.clustering.KMeans

// Loads data.
//val dataset = spark.read.option("header","true").option("inferSchema","true").csv("sample_kmeans_data.txt")
val dataset = spark.read.format("libsvm").option("numFeatures" ,"8").load("Wholesale customers data.csv")

// val kmeans = new KMeans().setK(4).setSeed(1L)
// val model = kmeans.fit(dataset)

// // Evaluate clustering by computing Within Set Sum of Squared Errors.
// val WSSSE = model.summary.trainingCost
// println(s"Within Set Sum of Squared Errors = $WSSSE")

// //Shows the result.
// println("Cluster Centers: ")
// model.clusterCenters.foreach(println)

import scala.collection.mutable.ListBuffer

val wssse_list = new ListBuffer[Double]()
for(k <- Range(2,8)){
val kmeans = new KMeans().setK(k).setSeed(1L)
val model = kmeans.fit(dataset)
val WSSSE = model.summary.trainingCost
wssse_list += (k,WSSSE)
//println(s"Within Set Sum of Squared Errors = $WSSSE")
}
wssse_list
