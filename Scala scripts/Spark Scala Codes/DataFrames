import org.apache.spark.sql.SparkSession
import spark.implicits._
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().getOrCreate()

//val df = spark.read.format("txt").option("header", "true").option("inferSchema" , "true").csv("airport.txt")

//df.show(3)

// for (row <- df.head(5))
    // println(row)
	
//df.describe().show()

//val df2 = df.withColumnRenamed("Sp.L" , "Sp_L").withColumnRenamed("Sp.W" , "Sp_W").withColumnRenamed("P.W","P_W").withColumnRenamed("P.L" , "P_L")
//val Sp_Low = df2.filter("Sp_L > 2 AND sp_W <3").collect()

//val df = df2

//groupBy and Agreggate functions

//df.groupBy("Species").mean().show(3)
//df.groupBy("Species").max().show()
//df.groupBy("Species").min().show()
//df.groupBy("Species").sum().show()
 
// df.select(countDistinct("Sp_L")).show()
// df.select(sumDistinct("Sp_L")).show()
// df.select(variance("Sp_L")).show()
// df.select(stddev("Sp_L")).show()
// df.select(collect_set("Sp_L")).show()

//df.show(3)
//df.orderBy($"Sp_L".desc).show(3)

//Missing data
val df = spark.read.format("txt").option("header", "true").option("inferSchema" , "true").csv("airport.txt")
//df.show(3)
//df.na.drop(2).show(3)
df.na.fill(100).show(3)

//date timestamp


