import org.apache.spark.sql.SparkSession
import spark.implicits._
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().getOrCreate()

val df = spark.read.format("txt").option("header", "true").option("inferSchema" , "true").csv("iris.txt")

//df.show(3)

// for (row <- df.head(5))
    // println(row)
	
//df.describe().show()

val df2 = df.withColumnRenamed("Sp.L" , "Sp_L").withColumnRenamed("Sp.W" , "Sp_W").withColumnRenamed("P.W","P_W").withColumnRenamed("P.L" , "P_L")

df2.filter("Sp_L > 2 AND sp_W <3").show(3)